{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61a087f",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f57270",
   "metadata": {},
   "source": [
    "Gradient Descent/Ascent (GD) uses repeatedly subtracts the gradient of the loss function to navigate the solution towards the local optimum. It Can struggle if the objective function is highly stochstic or if it has multiple local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97550bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d4c7a",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98247c",
   "metadata": {},
   "source": [
    "The drag per second a vehicle travelling on a road experiences is approximately R(v) = Av^2 + Bv + C where A is a factor relating aerodynamics, B to rolling and drivetrain losses, and C is a constant load. If t is the time taken to arrive at a destination D, minimizing t x R will minimize the losses due to drag for the journey. Since v = D/t and D is constant this is equivalent to minimizing R/v, i.e. Av + B + C/v.\n",
    "\n",
    "Suppose we have a car with A = 0.6, B = 5, C = 80, we have a well-defined optimization problem. It's fun to play around with A/B/C and see how different physical parameters affect the optimum speed. most cars travel ~30mph most of the time, so we expect an answer near 13m/s\n",
    "\n",
    "If you want a different objective function, check out Objectives.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a56725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dragMinimization(\n",
    "    hypothesis,\n",
    "    aerodynamicsConstant = 0.6,\n",
    "    rollingConstant = 5,\n",
    "    loadConstant = 80\n",
    "):\n",
    "    v = hypothesis[0]\n",
    "    aTerm = aerodynamicsConstant * v \n",
    "    bTerm = rollingConstant\n",
    "    cTerm = loadConstant / v\n",
    "    output = - (aTerm + bTerm + cTerm)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f711a11",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c405be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.54700558672454]\n"
     ]
    }
   ],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ranges\n",
    "    ):\n",
    "        self.ranges = ranges\n",
    "    \n",
    "    def optimize(self, objectiveFunction, numIterations = 25000, dx = 10 ** -10, learningRate = 0.01):\n",
    "        currentX = [r.uniform(x[0], x[1]) for x in self.ranges]\n",
    "        for i in range(numIterations):\n",
    "            currentY = objectiveFunction(currentX)\n",
    "            j = r.randint(0, len(self.ranges) - 1) #Updating in a random order can help escape local optima\n",
    "            placeholderX = currentX\n",
    "            placeholderX[j] += dx\n",
    "            newY = objectiveFunction(placeholderX)\n",
    "            currentX[j] += learningRate * ((newY - currentY) / dx)\n",
    "        return(currentX)\n",
    "gd = GradientDescent([[0, 150]])\n",
    "print(gd.optimize(dragMinimization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f5a8d-8078-4fa4-b1e4-4c71fb475b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
